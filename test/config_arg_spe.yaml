task:
  task: joint
  device: 0
  seed: 277
  clean_saved_models: true
  num_exemplars_per_class: 20
  total_num_exemplars: 10
  exemplars_grow_mode: "per_class"
  exemplar_algorithm: herding_class
  cal_num_exemplars_per_class_method: "by_class"
  smooth_factor: 2.0
  debug_mode: false
  coe_dist: false
data:
  dataset: cifar
  use_validation_set: false
  acc_val: true
  data_path: data
  store_in_memory: true
  num_batches: 20
  split: 10
  batch_size: 16
  test_batch_size: 512
  offline_batch_size: 128
  num_workers: 4
  pin_memory: true
  shuffle: true
  persistent_workers: true
  prompt: 1
  fold: 0
  w_len: 40
  e_len: -1
  word_vocab_args:
    size: 1800
    freq_threshold: 0
    grow_mode: "from_new"
    incremental_size: 200
  char_vocab_args:
    size: 1800
    freq_threshold: 0
    grow_mode: "from_new"
    incremental_size: 200
arch:
  model: resnet18
  pretrained: false
  emb_dim: 50
  char_encoder: Pyramid
  char_encoder_args:
    kernel: 7
    stride: 1
    pool_kernel: 2
    num_conv_per_layer: 1
    num_layers: 2
    dropout: 0
    compress: True
    conv_dim: 50
  word_encoder: Pyramid
  word_encoder_args:
    kernel: 7
    stride: 1
    pool_kernel: 2
    num_conv_per_layer: 1
    num_layers: 3
    dropout: 0
    compress: True
    conv_dim: 50
  char_emb_args:
    dim: 50
    freeze: false
    padding_idx: 0
    path: emb
    emb_type: "random"
  word_emb_args:
    dim: 50
    freeze: false
    padding_idx: 0
    path: emb
    emb_type: "pretrained"
train:
  trainer: ilol
  bic_select_online: false
  bic_no_freeze: false
  min_lr: 1.0e-4
  ol_lamb_mode: "eq"
  num_epochs: 50
  num_fine_tune_epochs: 1
  amp: false
  monitor: max train_acc
  performance_metric: test_acc
  no_batch_progress: true
  no_epoch_progress: true
  loss_args:
    alpha: 0.5
    beta: 0.5
    margin: 0
    temperature: 2
    tradeoff: 1000000.0
    forget_margin: 0
    mr_margin: 0.5
    dist_type: "less_forget"
    ranking_type: "old_new_intra"
    reject_order: "linear"
    t: 0.05
    K: 2
    fixed_lambda: 1.0
    fixed_dist_ratio: 0.5
    suppress_factor_at_2t: 0.5
    coe_dist: false
    factor_pool: 1
    factor_feat: 3
  minimum_metric: 0
  plateau_metric: test_loss
  lr_scheduler: ReduceLROnPlateau
  lr_scheduler_args:
    factor: 0.1
    gamma: 0.2
    step_size: 10
    milestones:
    - 60
    - 90
    - 120
    min_lr: 0
    mode: min
    patience: 10
    verbose: true
  optimizer: SGD
  optimizer_args:
    lr: 0.1
    momentum: 0.9
    weight_decay: 0.0001
  offline_mode: reduce
  sigma: 0.2
  regularization_type: "EWC"
  dev_train_ratio: 0.1
  lamb_base: 5.0
  lamb_mr: 1.0
  max_grad: 10000
log:
  comet_ml_key: u8zzPhVTZTFmGL3QZF4eTmjqE
  workspace: ol
  project_name: test
  experiment_name: nopretrained_nonormalize_scheduling
  offline: false
  offline_path: ~/comet_temp
  save_path: saved
  save_period: 500
  note: "none"
search:
  search_para: false
  op_config_path: configs/op.yaml
  trials: 5

_ARGUMENT_SPECIFICATION:
  task;task:
    flag: --task
  task;seed:
    flag: --seed
  task;device:
    flag: -d
  data;dataset:
    choices: ["cifar", "imagenet", "asap"]
  log;comet_ml_key:
    _IGNORE_IN_CLI
